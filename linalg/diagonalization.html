<!DOCTYPE html>
<html lang='en'>

<head>
    <base href="..">
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css"/>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=default">
    </script>
    <title>Diagonalization</title>
</head>

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
  * {box-sizing: border-box;}
  
  .header {
    overflow: hidden;
    background-color: #f8f8f885;
    padding: 20px 10px;
  }
  
  .header a {
    float: left;
    color: #050d38;
    text-align: center;
    padding: 12px;
    text-decoration: none;
    font-size: 18px; 
    line-height: 25px;
    border-radius: 4px;
  }
  
  .header a.logo {
    font-size: 25px;
    font-weight: bold;
  }
  
  .header a:hover {
    background-color: #ffffff;
    color: black;
  }
  
  .header a.active {
    background-color: #00baff;
    color: black;
  }
  
  .header-right {
    float: right;
  }
  
  @media screen and (max-width: 500px) {
    .header a {
      float: none;
      display: block;
      text-align: left;
    }
    
    .header-right {
      float: none;
    }
  }

  body{
  background-color: #f8f8f885;
  background-size: cover;
  background-attachment: fixed;
  color: #000000; 
  text-align:left;
  font-family:'Comic Sans MS';
  /*font-family:'Roboto',sans-serif;*/
  font-size:16pt;
  line-height:1.5em;
  margin: 60px auto;
  width: 1000px;
  }
  </style>
  </head>
    <body>
    
    <div class="header">
        <a class="active" href="index.html#notes">Back to Website</a>
    </div>

<body>
    <div>
      <ul style="list-style-type:none;">
        <li>
          <h4>
            Diagonalization
          </h4>
        </li>
        <li>
          Given a \(2\times 2\) real matrix \(A=\left(\begin{array}{cc}a&b\\c&d\end{array}\right)\), 
          suppose \(\lambda_1, \lambda_2\) are the roots (not necessarily distinct) of the equation
          \[det(A-\lambda I)=0\]
          then we can pick any non-zero vectors \(v_1, v_2\) such that \(Av_i=\lambda_iv_i\) respectively (if
          \(\lambda_1=\lambda_2\) then we also require \(v_1, v_2\) linearly independent). Define
          \[P=\left(\begin{array}{cc}|&|\\v_1&v_2\\|&|\end{array}\right)\]
          then we have
          \[A=PDP^{-1}\]
          where \(D=\left(\begin{array}{cc}\lambda_1&0\\0&\lambda_2\end{array}\right)\).
        </li>
      </ul>
    </div>
    <div>
      <ul style="list-style-type:none;">
        <li>
          <h4>
            Basic Explanation:
          </h4>
        </li>
        <li>
          The equation \(det(A-\lambda I)=0\) can be re-written into 
          \[
          \begin{align} 
          det \left(\begin{array}{cc}a-\lambda&b\\c&d-\lambda\end{array}\right)
          &= 0\\
          (a-\lambda)(d-\lambda)-bc&=0\\
          \lambda^2 - (a+d)\lambda +ad - bc &=0
          \end{align}
          \]
          whenever the discriminant \(\Delta = (a+d)^2-4(ad-bc)=(a-d)^2+bc\geq 0\), this quadratic equation
          has two real roots (probably repeating) and we say that \(A\) has (only) real eigenvalues. We do not consider
          the case of complex eigenvalues here.
        </li>
        <li>
          <h4>
            Example: 
          </h4>
        </li>
        <li>
          Consider the matrix \(A=\left(\begin{array}{cc}1&0\\2&4\end{array}\right)\). 
          \[det\left(\begin{array}{cc}1-\lambda&0\\2&4-\lambda\end{array}\right)=(1-\lambda)(4-\lambda)\]
          so the eigenvalues of \(A\) are \(\lambda_1=1, \lambda_2=4\) respectively.
        </li>
        <li>
          Let \(v_1=\left(\begin{array}{c}x\\y\end{array}\right)\) be a vector such that \(Av_1=\lambda_1v_1=1v_1=v_1\). Then we actually
          have
          \[\left(\begin{array}{cc}1&0\\2&4\end{array}\right)\left(\begin{array}{c}x\\y\end{array}\right)=
          \left(\begin{array}{c}x\\y\end{array}\right)\]
          By theory in system of linear equations, the above system is equivalent to 
          \[\left(\begin{array}{cc}0&0\\2&3\end{array}\right)\left(\begin{array}{c}x\\y\end{array}\right)=
          \left(\begin{array}{c}0\\0\end{array}\right)\]
          We can choose \(x=-3,y=2\). In this way we obtain a choice for \(v_1\).
        </li>
        <li>
          Similarly, the problem of finding \(v_2\) reduces to solving
          \[\left(\begin{array}{cc}-3&0\\2&0\end{array}\right)\left(\begin{array}{c}x\\y\end{array}\right)=
          \left(\begin{array}{c}0\\0\end{array}\right)\]
          and by observation \(x=0,y=1\) provides a solution.
        </li>
        <li>
          Write
          \[P=\left(\begin{array}{cc}-3&0\\2&1\end{array}\right)\]
          then we have
          \[A=\left(\begin{array}{cc}1&0\\2&4\end{array}\right)=\left(\begin{array}{cc}-3&0\\2&1\end{array}\right)
          \left(\begin{array}{cc}1&0\\0&4\end{array}\right)\left(\begin{array}{cc}-3&0\\2&1\end{array}\right)^{-1}\]
        </li>
        <li>
          <h4>
            The theory behind:
          </h4>
        </li>
        <li>
          \(\gamma=\{v_1, v_2\}\) forms an eigenbasis for \(\mathbb{R}^2\). Denote \(\beta\) the standard basis then
          \(P=[I]^\gamma_\beta\)
          where \(I\) is the identity transformation on \(\mathbb{R}^2\) and we are expressing \([I]_\gamma\) 
          in terms of \(\beta\). 
        </li>
        <li>
          As a quick reminder, given a vector representation \([v]_\beta\) in \(\beta\) we can representation it 
          in \(\gamma\) by 
          \[[v]_\gamma=[I]^\gamma_\beta [v]_\beta.\]
        </li>
        <li>
          We have \(AP=PD\) according to the following commutative diagram:
          $$\require{AMScd}
            \begin{CD}
            \left[\mathbb{R}^2\right]_\beta @>{A}>> \left[\mathbb{R}^2\right]_\beta\\
            @V{P}VV @V{P}VV \\
            \left[\mathbb{R}^2\right]_\gamma @>{D}>> \left[\mathbb{R}^2\right]_\gamma
            \end{CD}$$
          The same philosophy applies to general vector spaces of finite dimension.
        </li>
        <li>
          <h4>
            Remark on why we can choose \(v_i\) freely: 
          </h4>
        </li>
        <li>
          Suppose \(P, \tilde{P}\) are two matrices found by the same steps but different choices \(\tilde{v}_i\) from \(v_i\)
          we have \(\tilde{P}=PK\) for some matrix 
          \(K\) symbolizing some column operations. 
        </li>
        <li>
          However \(K\) is very special: whenever \(K\) symbolizes a column sum, that column sum must involve vectors from the same 
          eigenspace of dimension higher than 1.
        </li>
        <li>
          For example, if \(D\) looks like 
          \[\tilde{D}=\left(\begin{array}{ccc}\lambda_1&0&0\\0&\lambda_2&0\\0&0&\lambda_2\end{array}\right)\]
          and \(\tilde{D}\) looks like
          \[\tilde{D}=\left(\begin{array}{ccc}\lambda_2&0&0\\0&\lambda_2&0\\0&0&\lambda_1\end{array}\right)\]
          then \(K\) is a block matrix
          \[K=\left(\begin{array}{ccc}0&0&1\\*&*&0\\*&*&0\end{array}\right)\]
        </li>
        <li>
          More concretely, suppose \(A\) is a \(3\times 3\) square matrix with eigenvalues \(\lambda_1, \lambda_2, \lambda_3=\lambda_2\).
          This means \(\lambda_2\) is an eigenvalue of algebraic multiplicity 2, equivalently the factor \((\lambda-\lambda_2)^2\)
          has degree 2 in the characteristic polynomial of \(A\).
        </li>
        <li>
          Further suppose \(u_1, u_2, u_3\) are first chosen such that \(Au_1=\lambda_1u_1, Au_2=\lambda_2u_2, Au_3=\lambda_2u_3\).
          Also, \(u_2, u_3\) are linearly independent, in the sense that they form a basis for \(N(A-\lambda_2 I)\).
        </li>
        <li>
          The above information allows us to write down
          \[A=\left(\begin{array}{ccc}|&|&|\\u_1&u_2&u_3\\|&|&|\end{array}\right)
          \left(\begin{array}{ccc}\lambda_1&0&0\\0&\lambda_2&0\\0&0&\lambda_2\end{array}\right)
          \left(\begin{array}{ccc}|&|&|\\u_1&u_2&u_3\\|&|&|\end{array}\right)^{-1}\]
        </li>
        <li>
          Suppose now another choice is made, say \(\tilde{\lambda_1}=\lambda_2, \tilde{\lambda_2}=\lambda_2, \tilde{\lambda_3}=\lambda_1\). 
          This can happen when different student solve \(det (A-\lambda I)=0\) and write down different roots in different order, say.
        </li>
        <li>
          Then, suppose it is chosen that \(\tilde{u}_1=u_2-u_3, \tilde{u}_2=u_2+u_3, \tilde{u}_3=u_1\). We have
          \[
          \begin{align} 
          A&=\left(\begin{array}{ccc}|&|&|\\\tilde{u}_1&\tilde{u}_2&\tilde{u}_3\\|&|&|\end{array}\right)
          \left(\begin{array}{ccc}\tilde{\lambda}_1&0&0\\0&\tilde{\lambda}_2&0\\0&0&\tilde{\lambda}_3\end{array}\right)
          \left(\begin{array}{ccc}|&|&|\\\tilde{u}_1&\tilde{u}_2&\tilde{u}_3\\|&|&|\end{array}\right)^{-1} \\
          &= \left(\begin{array}{ccc}|&|&|\\u_2-u_3&u_2+u_3&u_1\\|&|&|\end{array}\right)
          \left(\begin{array}{ccc}\lambda_2&0&0\\0&\lambda_2&0\\0&0&\lambda_1\end{array}\right)
          \left(\begin{array}{ccc}|&|&|\\u_2-u_3&u_2+u_3&u_1\\|&|&|\end{array}\right)^{-1} \\
          &= \left(\begin{array}{ccc}|&|&|\\u_1&u_2&u_3\\|&|&|\end{array}\right)
          \left(\begin{array}{ccc}0&0&1\\1&1&0\\-1&1&0\end{array}\right)
          \left(\begin{array}{ccc}\lambda_2&0&0\\0&\lambda_2&0\\0&0&\lambda_1\end{array}\right)
          \left(\begin{array}{ccc}0&0&1\\1&1&0\\-1&1&0\end{array}\right)^{-1}
          \left(\begin{array}{ccc}|&|&|\\u_1&u_2&u_3\\|&|&|\end{array}\right)^{-1}\\
          &= \left(\begin{array}{ccc}|&|&|\\u_1&u_2&u_3\\|&|&|\end{array}\right)
          \left(\begin{array}{ccc}\lambda_1&0&0\\0&\lambda_2&0\\0&0&\lambda_2\end{array}\right)
          \left(\begin{array}{ccc}|&|&|\\u_1&u_2&u_3\\|&|&|\end{array}\right)^{-1}
          \end{align}
          \]
        </li>
        <li>
          Therefore, even though in general diagonal matrices does not commute with invertible matrices, we still have
          \(KDK^{-1}=D\) (hint: \(\lambda I\) commutes with every matrix!) where \(D\) is a diagonal matrix of eigenvalues.
          \[\tilde{P}D\tilde{P}^{-1}=PKDK^{-1}P^{-1}=PDP^{-1}.\]
        </li>
      </ul>
    </div>
</body>
</body>
</html>